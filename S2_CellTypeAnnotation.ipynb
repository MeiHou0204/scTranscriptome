{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e51d5d",
   "metadata": {},
   "source": [
    "## We evaluate the cell type annotation performance of the pretrained model, \n",
    "## using a multilayer perceptron (MLP) as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset,DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix,f1_score\n",
    "import ray\n",
    "from ray import train,tune\n",
    "from ray.tune import CLIReporter, Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle\n",
    "import inspect\n",
    "import json\n",
    "from MachineLearnFunc import SparseToDenseDataset, HyperparameterTune,load_config\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "work_dir='./'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90bc41",
   "metadata": {},
   "source": [
    "### step 1. load dataset.\n",
    "#### This dataset download from cellxgene, which contain 156,726 single cell RNA sequencing of the human embryonic meninges at 5-13 weeks post conception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d872d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset download from here: https://datasets.cellxgene.cziscience.com/f82c6e2b-8056-45a3-967a-fbadf057566f.h5ad\n",
    "filepath=f\"{work_dir}/datasets/f82c6e2b-8056-45a3-967a-fbadf057566f.h5ad\"\n",
    "sc_data = sc.read_h5ad(filepath)##load file as anndata object,n_obs × n_vars = 156726 × 33159\n",
    "print(sc_data)\n",
    "##filter low-quality cells and genes, normalize the data, and identify highly variable genes\n",
    "sc_data.obs['fraction_mitochondrial']#(min,median,max)=(0,0.04,0.82)\n",
    "sc_data.obs['total_UMIs']#(min,median,max)=(1000,7057,290169)\n",
    "sc_data.obs['total_genes']#(min,median,max)=(39,2677,14076)\n",
    "sc_data = sc_data[sc_data.obs['fraction_mitochondrial'] < 0.2, :] #filter out cells with >20% mitochondrial genes\n",
    "sc.pp.filter_cells(sc_data, min_genes=200)#filter out cells with <200 genes\n",
    "sc.pp.filter_cells(sc_data, max_genes=6000) #filter out cells with >6000 genes\n",
    "# sc.pp.filter_genes(sc_data, min_cells=3)#filter out genes expressed in <3 cells\n",
    "##generally, we need split dataset before normalization,\n",
    "##but in single cell RNA-seq data, the normalization method is based on the total counts of each cell\n",
    "sc.pp.normalize_total(sc_data, target_sum=1e4)##normalize each cell by total counts over all genes, so that every cell has the same total count after normalization (1e4)\n",
    "sc.pp.log1p(sc_data)##log-transform the data\n",
    "# sc.pp.highly_variable_genes(sc_data, n_top_genes=3000, flavor='seurat') ##identify the top 3000 highly variable genes\n",
    "# sc_data = sc_data[:, sc_data.var.highly_variable]\n",
    "\n",
    "# encoding labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(sc_data.obs['cell_type']) ##only for learning encoding way in training set\n",
    "num_classes=len(label_encoder.classes_)  \n",
    "print(f'Number of cell types: {num_classes}')# 58 cell types\n",
    "y = label_encoder.transform(sc_data.obs['cell_type'])\n",
    "\n",
    "# to split the data into training, validation, and test sets, while save memory\n",
    "# to extract dense tensors from sparse matrices,use a custom dataset class-->SparseToDenseDataset\n",
    "# split the indices instead of the data itself, so that we don't need to create multiple copies of the data in memory.\n",
    "all_indices = np.arange(sc_data.obs.shape[0])\n",
    "train_dev_indices, test_indices, y_train_dev, y_test = train_test_split(all_indices,y, test_size=0.2, random_state=42, stratify=y)\n",
    "train_indices, dev_indices, y_train, y_dev = train_test_split(train_dev_indices,y_train_dev,test_size=0.3, random_state=42, stratify=y_train_dev)\n",
    "print('Data prepared!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ba1ea",
   "metadata": {},
   "source": [
    "### step 2. Cell type annotation with MLP model as baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dcb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a simple MLP classifier\n",
    "class CellTypeClassifierLinear(nn.Module):\n",
    "    def __init__(self,  num_classes=num_classes,\n",
    "                 input_dim=sc_data.X.shape[1],hidden_neuron=200,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_neuron),  # embed_dim=input_dim to hidden_neuron\n",
    "            nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_neuron, num_classes)  # output cell types prediction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x.squeeze(1)) \n",
    "\n",
    "## training function for single trial\n",
    "def Train_CellType_Annotation(config,patience_=5):\n",
    "    ## load datasets from Ray object store (instead of capturing directly)\n",
    "    train_dataset = ray.get(config[\"train_data_ref\"])\n",
    "    dev_dataset = ray.get(config[\"dev_data_ref\"])\n",
    "    if config.get(\"use_subset\", True):\n",
    "        ## prepare a subset of training data for quick tuning\n",
    "        subset_indices = np.random.choice(len(train_dataset), int(config['subset_fraction'] * len(train_dataset)),replace=False)\n",
    "        train_subset = Subset(train_dataset, subset_indices)\n",
    "    else:\n",
    "        train_subset = train_dataset\n",
    "    ## model initialization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = config['classifier'](hidden_neuron=config[\"hidden_neuron\"])\n",
    "    model.to(device)\n",
    "    ## define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #load data in batches\n",
    "    train_loader = DataLoader(train_subset, batch_size=config[\"batch_size\"],shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset,batch_size=config[\"batch_size\"])\n",
    "    ## training cycle with Early Stopping\n",
    "    best_f1 = 0.0  ## best_f1 starts from 0, range from 0 to 1\n",
    "    no_improve = 0\n",
    "    checkpoint = None  ## initialize checkpoint\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        ## average train loss per batch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        ## evaluate loss on dev set\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in dev_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                logits = model(batch_X)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_true.append(batch_y.cpu())\n",
    "\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_true = torch.cat(all_true).numpy()\n",
    "\n",
    "        dev_f1 = f1_score(all_true, all_preds, average='macro') # calculate F1 score (macro)\n",
    "        # print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Dev F1={dev_f1:.4f}, Best F1={best_f1:.4f}\") #for test\n",
    "\n",
    "        metrics = {\n",
    "            'train_loss': float(avg_train_loss),\n",
    "            'dev_f1': float(dev_f1),\n",
    "            'best_f1': float(best_f1),\n",
    "            'epoch': epoch + 1}\n",
    "\n",
    "        ## Early Stopping and Checkpointing\n",
    "        checkpoint_frequency = 5  ## save checkpoint every 5 epochs\n",
    "        if dev_f1 > best_f1:\n",
    "            best_f1 = dev_f1\n",
    "            no_improve = 0 ##reset no improve,still improving\n",
    "             ## save checkpoint\n",
    "            if (epoch + 1) % checkpoint_frequency == 0:\n",
    "                with tempfile.TemporaryDirectory() as tempdir:\n",
    "                    ## save model parameters\n",
    "                    torch.save(model.state_dict(), os.path.join(tempdir, \"model.pt\"))\n",
    "                    ## save extra info to a pickle file\n",
    "                    checkpoint_data = metrics\n",
    "                    with open(os.path.join(tempdir, \"checkpoint_data.pkl\"), \"wb\") as f:\n",
    "                        pickle.dump(checkpoint_data, f)\n",
    "                    ## create a Checkpoint object from the directory\n",
    "                    checkpoint = Checkpoint.from_directory(tempdir)\n",
    "                    ## report checkpoint to Ray Tune\n",
    "                    tune.report(metrics, checkpoint=checkpoint)\n",
    "            else:\n",
    "                tune.report(metrics)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            tune.report(metrics)\n",
    "            if no_improve >= patience_:\n",
    "                break  ## early stopping\n",
    "\n",
    "dataset_ratio=['0.01','0.05','1.00']\n",
    "for i in dataset_ratio:\n",
    "    os.makedirs(f\"{work_dir}/CellTypeAnnotation/linear{i}\", exist_ok=True)\n",
    "    ## search for best hyperparameters\n",
    "    config={'classifier':CellTypeClassifierLinear,\n",
    "        'TrainingFunc':Train_CellType_Annotation,\n",
    "        'label_names':label_encoder.classes_,\n",
    "        'metriclst':[\"train_loss\", \"dev_f1\", \"best_f1\", \"epoch\"],\n",
    "        'metric_standard':'best_f1',\n",
    "        'metric_mode':'max',\n",
    "        'num_epochs':10,\n",
    "        'num_epochs_atleast':5,\n",
    "        'totaltrials':10,\n",
    "        'learning_rate':tune.loguniform(1e-5, 1e-3),\n",
    "        'batch_size':500,#tune.choice([500,1000]),\n",
    "        'hidden_neuron':400,#tune.choice([1000,2000]),\n",
    "        \"use_subset\":True,\n",
    "        'subset_fraction':float(i),\n",
    "        'outputPath':f'{work_dir}/CellTypeAnnotation/linear{i}',##ginpai\n",
    "        'filename':\"scCellTypeAnnotation_tune\",}\n",
    "\n",
    "    ## use a custom hyperparameter tuning function to launch the tuning process\n",
    "    HyperparameterTune(anndata_obj=sc_data,y=y,train_indices=train_indices,dev_indices=dev_indices,test_indices=test_indices,config=config,initcpu=6,initgpu=1,onetrialcpu=1,onetrialgpu=0.2,evaluate_classifier=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af4d1ac",
   "metadata": {},
   "source": [
    "### step 3. Cell type annotation with pretrained model+MLP (transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0687e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load a decoder-only pretrained model\n",
    "class SingleCellDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = sc_data.X.shape[1],   # how many genes in a cell,columns of the data matrix\n",
    "        hidden_dim: int = 512,   # neuron number in the hidden layer\n",
    "        num_layers: int = 2,     # layers of Transformer\n",
    "        nhead: int = 4,          # number of attention heads\n",
    "        dropout: float = 0.1,    # Dropout rate\n",
    "        output_dim: int = sc_data.X.shape[1]   # ,output_dim,default to input_dim for reconstruction\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding layer, embed the input gene expression to a higher-dimensional space\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # one decoder-only layer,\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )## batch_first:tell function input shape.True = (batch_size, seq_length,features_dimension).False = (seq_length,batch_size, features_dimension)\n",
    "        \n",
    "        # multiple decoder-only layer\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        # output layer, hidden layer to gene expression reconstruction.\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input: x: (batch_size, seq_len, input_dim)  # single cell gene expression matrix\n",
    "        output: (batch_size, seq_len, input_dim)     # recontruct gene expression matrix\n",
    "        \"\"\"\n",
    "        # 1. embedding\n",
    "        x_embed = self.embedding(x)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 2. self-attention decoder-only layer\n",
    "        memory = torch.zeros_like(x_embed)  # without encoder, only autoregression\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=x_embed, ## embeded input\n",
    "            memory=memory,## without encoder, use zero to occupied memory\n",
    "        )##output shape (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 3. output reconstructed gene expression matrix\n",
    "        return self.output_layer(output)\n",
    "## load pretrained model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model= SingleCellDecoder(load_config(f\"{work_dir}/Pretrained_model/best_model_hyperparameters.json\"))\n",
    "pretrained_model.load_state_dict(torch.load(f\"{work_dir}/Pretrained_model/best_model.pth\",map_location=device))\n",
    "pretrained_model.to(device)\n",
    "\n",
    "## define a transfer learning classifier\n",
    "class CellTypeClassifierTransfer(nn.Module):\n",
    "    def __init__(self,  num_classes=num_classes, encoder=pretrained_model,\n",
    "                 input_dim=sc_data.X.shape[1],hidden_neuron=200,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder  # pretrained model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_neuron),\n",
    "            nn.ReLU(),nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_neuron, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.encoder(x)\n",
    "        # print(embeddings.squeeze(1).shape)  # remove seq_len dimension → [batch_size, 2000]\n",
    "        return self.classifier(embeddings.squeeze(1)) ## prediting cell types\n",
    "\n",
    "dataset_ratio=['0.01','0.05','1.00']\n",
    "for i in dataset_ratio:\n",
    "    os.makedirs(f\"{work_dir}/CellTypeAnnotation/transfer{i}\", exist_ok=True)\n",
    "    ##hyperparameter tuning: search space\n",
    "    config={'classifier':CellTypeClassifierTransfer,\n",
    "        'TrainingFunc':Train_CellType_Annotation,\n",
    "        'label_names':label_encoder.classes_,\n",
    "        'metriclst':[\"train_loss\", \"dev_f1\", \"best_f1\", \"epoch\"],\n",
    "        'metric_standard':'best_f1',\n",
    "        'metric_mode':'max',\n",
    "        'num_epochs':10,\n",
    "        'num_epochs_atleast':5,\n",
    "        'totaltrials':10,\n",
    "        'learning_rate':tune.loguniform(1e-5, 1e-3),\n",
    "        'batch_size':500,#tune.choice([500,1000]),\n",
    "        'hidden_neuron':400,#tune.choice([400, 800]),\n",
    "        \"use_subset\":True,\n",
    "        'subset_fraction':float(i),\n",
    "        'outputPath':f\"{work_dir}/CellTypeAnnotation/transfer{i}\",\n",
    "        'filename':\"scCellTypeAnnotation_tune\",} \n",
    "\n",
    "    HyperparameterTune(anndata_obj=sc_data,y=y,train_indices=train_indices,dev_indices=dev_indices,test_indices=test_indices,config=config,initcpu=6,initgpu=1,onetrialcpu=1,onetrialgpu=0.2,evaluate_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2406fe",
   "metadata": {},
   "source": [
    "### step 4. comparing cell type annotation performance between transfer learning and MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##collect results from different trials and make plots\n",
    "##rename suffix of those files mannually\n",
    "os.makedirs(f\"{work_dir}/evaluation\", exist_ok=True)\n",
    "suffix_lst=['linear1perc','linear5perc','linear100perc','tl1perc','tl5perc','tl100perc']\n",
    "group_lst=['mlp','mlp','mlp','transfer learning','transfer learning','transfer learning']\n",
    "subset_prop=['0.01','0.05','1.00','0.01','0.05','1.00']\n",
    "sumdf=pd.DataFrame()\n",
    "for i in range(len(suffix_lst)):\n",
    "    cta_tp=pd.read_csv(f\"{work_dir}/evaluation/classification_report_{suffix_lst[i]}.csv\",index_col=0)\n",
    "    cta_tp=cta_tp.iloc[:-3,:]\n",
    "    cta_tp=cta_tp.assign(group=group_lst[i],subset_proportion=subset_prop[i])\n",
    "    sumdf=pd.concat([sumdf,cta_tp],axis=0)\n",
    "\n",
    "sumdf.index.name='cell type'\n",
    "## table wide to long format for seaborn plotting\n",
    "df_long = sumdf.reset_index().melt(\n",
    "    id_vars=['cell type', 'support', 'group', 'subset_proportion'],\n",
    "    value_vars=['precision', 'recall', 'f1-score'],\n",
    "    var_name='metric',\n",
    "    value_name='score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "# create 1x3 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# create boxplots for each metric\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "titles = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    metric_data = df_long[df_long['metric'] == metric]\n",
    "    sns.boxplot(\n",
    "        data=metric_data, \n",
    "        x='subset_proportion', \n",
    "        y='score', \n",
    "        hue='group',\n",
    "        ax=axes[i],\n",
    "        palette=['#7fbc41', '#de77ae'])\n",
    "    \n",
    "    # axes[i].set_title(f'{title}', fontsize=14)\n",
    "    axes[i].set_xlabel('Training Set Proportion',   fontsize=14)\n",
    "    axes[i].set_ylabel(f'{title}', fontsize=14)\n",
    "    axes[i].legend(title='Method',  fontsize=12,loc='lower right')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Performance Across Different Training Set Proportions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{work_dir}/evaluation/boxplot.jpg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
