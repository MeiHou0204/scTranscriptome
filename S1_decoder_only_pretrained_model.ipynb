{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e51d5d",
   "metadata": {},
   "source": [
    "## We evaluate the cell type annotation performance of the pretrained model, \n",
    "## using a multilayer perceptron (MLP) as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset,DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix,f1_score\n",
    "import ray\n",
    "from ray import train,tune\n",
    "from ray.tune import CLIReporter, Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle\n",
    "import inspect\n",
    "from MachineLearnFunc import SparseToDenseDataset, HyperparameterTune\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "work_dir='./'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90bc41",
   "metadata": {},
   "source": [
    "### step 1. load dataset.\n",
    "#### This dataset download from cellxgene, which contain 156,726 single cell RNA sequencing of the human embryonic meninges at 5-13 weeks post conception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d872d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset download from here: https://datasets.cellxgene.cziscience.com/f82c6e2b-8056-45a3-967a-fbadf057566f.h5ad\n",
    "filepath=f\"{work_dir}/datasets/f82c6e2b-8056-45a3-967a-fbadf057566f.h5ad\"\n",
    "sc_data = sc.read_h5ad(filepath)##load file as anndata object,n_obs × n_vars = 156726 × 33159\n",
    "print(sc_data)\n",
    "##filter low-quality cells and genes, normalize the data, and identify highly variable genes\n",
    "sc_data.obs['fraction_mitochondrial']#(min,median,max)=(0,0.04,0.82)\n",
    "sc_data.obs['total_UMIs']#(min,median,max)=(1000,7057,290169)\n",
    "sc_data.obs['total_genes']#(min,median,max)=(39,2677,14076)\n",
    "sc_data = sc_data[sc_data.obs['fraction_mitochondrial'] < 0.2, :] #filter out cells with >20% mitochondrial genes\n",
    "sc.pp.filter_cells(sc_data, min_genes=200)#filter out cells with <200 genes\n",
    "sc.pp.filter_cells(sc_data, max_genes=6000) #filter out cells with >6000 genes\n",
    "# sc.pp.filter_genes(sc_data, min_cells=3)#filter out genes expressed in <3 cells\n",
    "##generally, we need split dataset before normalization,\n",
    "##but in single cell RNA-seq data, the normalization method is based on the total counts of each cell\n",
    "sc.pp.normalize_total(sc_data, target_sum=1e4)##normalize each cell by total counts over all genes, so that every cell has the same total count after normalization (1e4)\n",
    "sc.pp.log1p(sc_data)##log-transform the data\n",
    "# sc.pp.highly_variable_genes(sc_data, n_top_genes=3000, flavor='seurat') ##identify the top 3000 highly variable genes\n",
    "# sc_data = sc_data[:, sc_data.var.highly_variable]\n",
    "\n",
    "# encoding labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(sc_data.obs['cell_type']) ##only for learning encoding way in training set\n",
    "num_classes=len(label_encoder.classes_)  \n",
    "print(f'Number of cell types: {num_classes}')# 58 cell types\n",
    "y = label_encoder.transform(sc_data.obs['cell_type'])\n",
    "\n",
    "# to split the data into training, validation, and test sets, while save memory\n",
    "# to extract dense tensors from sparse matrices,use a custom dataset class-->SparseToDenseDataset\n",
    "# split the indices instead of the data itself, so that we don't need to create multiple copies of the data in memory.\n",
    "all_indices = np.arange(sc_data.obs.shape[0])\n",
    "train_dev_indices, test_indices, y_train_dev, y_test = train_test_split(all_indices,y, test_size=0.2, random_state=42, stratify=y)\n",
    "train_indices, dev_indices, y_train, y_dev = train_test_split(train_dev_indices,y_train_dev,test_size=0.3, random_state=42, stratify=y_train_dev)\n",
    "print('Data prepared!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48458be4",
   "metadata": {},
   "source": [
    "### step 2. Training a decoder-only pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1: define a decoder-only model \n",
    "class SingleCellDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = sc_data.X.shape[1],   # how many genes in a cell,columns of the data matrix\n",
    "        hidden_dim: int = 512,   # neuron number in the hidden layer\n",
    "        num_layers: int = 2,     # layers of Transformer\n",
    "        nhead: int = 4,          # number of attention heads\n",
    "        dropout: float = 0.1,    # Dropout rate\n",
    "        output_dim: int = sc_data.X.shape[1]   # ,output_dim,default to input_dim for reconstruction\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding layer, embed the input gene expression to a higher-dimensional space\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # one decoder-only layer,\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )## batch_first:tell function input shape.True = (batch_size, seq_length,features_dimension).False = (seq_length,batch_size, features_dimension)\n",
    "        \n",
    "        # multiple decoder-only layer\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        # output layer, hidden layer to gene expression reconstruction.\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input: x: (batch_size, seq_len, input_dim)  # single cell gene expression matrix\n",
    "        output: (batch_size, seq_len, input_dim)     # recontruct gene expression matrix\n",
    "        \"\"\"\n",
    "        # 1. embedding\n",
    "        x_embed = self.embedding(x)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 2. self-attention decoder-only layer\n",
    "        memory = torch.zeros_like(x_embed)  # without encoder, only autoregression\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=x_embed, ## embeded input\n",
    "            memory=memory,## without encoder, use zero to occupied memory\n",
    "        )##output shape (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 3. output reconstructed gene expression matrix\n",
    "        return self.output_layer(output)\n",
    "\n",
    "\n",
    "## 2.2: tune hyperparameters \n",
    "def SingleCell_Pretrain(config,patience_=5):\n",
    "    ## 1. prepare data\n",
    "    train_dataset = ray.get(config[\"train_data_ref\"])\n",
    "    dev_dataset = ray.get(config[\"dev_data_ref\"])\n",
    "    if config.get(\"use_subset\", True):\n",
    "        subset_indices = np.random.choice(len(train_dataset), int(config['subset_proportion'] * len(train_dataset)),replace=False)\n",
    "        train_subset = Subset(train_dataset, subset_indices)\n",
    "    else:\n",
    "        train_subset = train_dataset\n",
    "    \n",
    "    # 2. model initialization\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = config['classifier'](\n",
    "        nhead=config[\"nhead\"],\n",
    "        num_layers=config['num_layers']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 3. data loaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=config[\"batch_size\"],shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset,batch_size=config[\"batch_size\"])\n",
    "\n",
    "    # 4. training loop with Early Stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    checkpoint = None\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        ##training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, _ in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_x = model(batch_X)\n",
    "            loss = criterion(pred_x, batch_X)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        # validation phase\n",
    "        model.eval()\n",
    "        dev_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, _ in dev_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                pred_x = model(batch_X)\n",
    "                dev_loss += criterion(pred_x, batch_X).item()\n",
    "        \n",
    "        dev_loss /= len(dev_loader)\n",
    "        \n",
    "        metrics = {\n",
    "            'train_loss': float(train_loss),\n",
    "            'dev_loss': float(dev_loss),\n",
    "            'best_loss': float(best_loss),\n",
    "            'epoch': epoch + 1,\n",
    "        }\n",
    "\n",
    "        # Early Stopping and Checkpointing\n",
    "        checkpoint_frequency = 5  # save a checkpoint every 5 epochs\n",
    "        if dev_loss < best_loss:\n",
    "            best_loss = dev_loss\n",
    "            no_improve = 0 ##reset no improve,still improving\n",
    "             # create a temporary directory to save checkpoints\n",
    "            if (epoch + 1) % checkpoint_frequency == 0:\n",
    "                with tempfile.TemporaryDirectory() as tempdir:\n",
    "                    # save model parameters\n",
    "                    torch.save(model.state_dict(), os.path.join(tempdir, \"model.pt\"))\n",
    "                    # save extra info to a pickle file\n",
    "                    checkpoint_data = metrics\n",
    "                    with open(os.path.join(tempdir, \"checkpoint_data.pkl\"), \"wb\") as f:\n",
    "                        pickle.dump(checkpoint_data, f)\n",
    "                    # create a Checkpoint object from the temporary directory\n",
    "                    checkpoint = Checkpoint.from_directory(tempdir)# wrap up this dir as Checkpoint,\n",
    "                    # report metrics and checkpoint to Tune\n",
    "                    tune.report(metrics, checkpoint=checkpoint)\n",
    "            else:\n",
    "                tune.report(metrics)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            tune.report(metrics)\n",
    "            if no_improve >= patience_:\n",
    "                break  \n",
    "\n",
    "os.makedirs(f\"{work_dir}/Pretrained_model\", exist_ok=True)\n",
    "## search for best hyperparameters\n",
    "config={'classifier':SingleCellDecoder,\n",
    "        'TrainingFunc':SingleCell_Pretrain,\n",
    "        'metriclst':['train_loss', 'dev_loss', 'best_loss', 'epoch'],\n",
    "        'metric_standard':'best_loss',\n",
    "        'metric_mode':'min',\n",
    "        'num_epochs':50,\n",
    "        'num_epochs_atleast':10,\n",
    "        'totaltrials':50,\n",
    "        'learning_rate':tune.loguniform(1e-5, 1e-3),\n",
    "        'batch_size':tune.choice([500,1000]),\n",
    "        \"num_layers\": tune.choice([4,8]),\n",
    "        \"nhead\": tune.choice([4,8]),\n",
    "        \"use_subset\":True,\n",
    "        \"subset_proportion\":0.2,\n",
    "        'outputPath':f\"{work_dir}/Pretrained_model\",\n",
    "        'filename':\"scPretrain_tune\",}\n",
    "\n",
    "## use a custom hyperparameter tuning function to launch the tuning process\n",
    "HyperparameterTune(anndata_obj=sc_data,y=y,train_indices=train_indices,dev_indices=dev_indices,test_indices=test_indices,config=config,initcpu=6,initgpu=1,onetrialcpu=1,onetrialgpu=0.2,)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
